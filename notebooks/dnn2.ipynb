{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/ALLDATA_v2.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN for computing Pos_avgs\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Select features for KNN\n",
    "features = ['HS', 'AS', 'HST', 'AST', 'Hpts', 'Apts', 'Home_Form_Points', 'Away_Form_Points']\n",
    "target_columns = [\"HTPos_avg\", \"ATPos_avg\"]\n",
    "\n",
    "# Create a mask to identify rows with missing values\n",
    "missing_mask = df[target_columns].isnull()\n",
    "\n",
    "# Add missingness indicators to the DataFrame\n",
    "for col in target_columns:\n",
    "    df[f\"{col}_missing\"] = missing_mask[col].astype(int)\n",
    "\n",
    "# Create a copy of the data for imputation\n",
    "imputation_data = df[features + target_columns].copy()\n",
    "\n",
    "# Apply KNN imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "imputed_data = knn_imputer.fit_transform(imputation_data)\n",
    "\n",
    "# Convert imputed data back to a DataFrame\n",
    "imputed_df = pd.DataFrame(imputed_data, columns=features + target_columns)\n",
    "\n",
    "# Replace only the missing values in the original DataFrame\n",
    "for col in target_columns:\n",
    "    df.loc[missing_mask[col], col] = imputed_df.loc[missing_mask[col], col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Visualize imputed vs. non-imputed values\n",
    "# plt.hist(df['HTPos_avg'], bins=30, alpha=0.5, label='Home Pos Avg')\n",
    "# plt.hist(df.loc[missing_mask['HTPos_avg'], 'HTPos_avg'], bins=30, alpha=0.5, label='Imputed Home Pos Avg')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK IF THIS IS ACTUALLY USEFUL TO INCLUDE, FEELS LIKE ACCURACY DIPS SOMETIMES BECAUSE OF THIS AS THERE'S AN OBSERVED FLUCTUATION IN ACCURACY BETWEEN 0.46 AND 0.53 \n",
    "# Use a combination of random forest regressor and an iterative imputer to get missing values for HSPE and ASPE\n",
    "\n",
    "# Random Forest Regression\n",
    "def random_forest_impute(df, target_col, feature_cols):\n",
    "    \"\"\"\n",
    "    Trains a RandomForestRegressor to predict 'target_col' using 'feature_cols'.\n",
    "    Fills in missing values in 'target_col' in the original df.\n",
    "    \"\"\"\n",
    "    not_missing_mask = df[target_col].notnull()\n",
    "    missing_mask = df[target_col].isnull()\n",
    "\n",
    "    df_not_missing = df[not_missing_mask]\n",
    "    df_missing = df[missing_mask]\n",
    "\n",
    "    if df_missing.empty:\n",
    "        print(f\"No missing values for {target_col}. Skipping RF imputation.\")\n",
    "        return df\n",
    "\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=20,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    rf.fit(df_not_missing[feature_cols], df_not_missing[target_col])\n",
    "    imputed_values = rf.predict(df_missing[feature_cols])\n",
    "    df.loc[missing_mask, target_col] = imputed_values\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example feature set for random forest:\n",
    "rf_features = [\n",
    "    'Hpts', 'Apts', \n",
    "    'Home_Form_Points', 'Away_Form_Points',\n",
    "    'Home_H2H_Win_Rate', 'Away_H2H_Win_Rate',\n",
    "    'HTS', 'ATS'\n",
    "]\n",
    "\n",
    "df[\"HSPE_missing\"] = df[\"HSPE (%)\"].isnull().astype(int)\n",
    "df[\"ASPE_missing\"] = df[\"ASPE (%)\"].isnull().astype(int)\n",
    "\n",
    "df = random_forest_impute(\n",
    "    df=df, \n",
    "    target_col='HSPE (%)', \n",
    "    feature_cols=rf_features\n",
    ")\n",
    "\n",
    "df = random_forest_impute(\n",
    "    df=df, \n",
    "    target_col='ASPE (%)', \n",
    "    feature_cols=rf_features\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest followed by iterated imputation to be able to get missing values for HPE AND APE and include them as features\n",
    "df[\"HPE_missing\"] = df[\"HPE (%)\"].isnull().astype(int)\n",
    "df[\"APE_missing\"] = df[\"APE (%)\"].isnull().astype(int)\n",
    "\n",
    "rf_features_for_hpe_ape = [\n",
    "    'Hpts', 'Apts',\n",
    "    'Home_Form_Points', 'Away_Form_Points',\n",
    "    'Home_H2H_Win_Rate', 'Away_H2H_Win_Rate',\n",
    "    'HTS', 'ATS',\n",
    "]\n",
    "\n",
    "df = random_forest_impute(\n",
    "    df=df,\n",
    "    target_col='HPE (%)',\n",
    "    feature_cols=rf_features_for_hpe_ape\n",
    ")\n",
    "\n",
    "df = random_forest_impute(\n",
    "    df=df,\n",
    "    target_col='APE (%)',\n",
    "    feature_cols=rf_features_for_hpe_ape\n",
    ")\n",
    "\n",
    "impute_cols = (\n",
    "    rf_features_for_hpe_ape + \n",
    "    [\"HPE (%)\", \"APE (%)\"]\n",
    ")\n",
    "\n",
    "# Remove duplicates from the combined list\n",
    "impute_cols = list(dict.fromkeys(impute_cols))\n",
    "\n",
    "# We only keep these columns in a separate DataFrame\n",
    "iter_data = df[impute_cols].copy()\n",
    "\n",
    "# Keep original features\n",
    "original_features = df[impute_cols].copy()  # or just keep the rf_features part\n",
    "\n",
    "# Initialize IterativeImputer\n",
    "iter_imputer = IterativeImputer(\n",
    "    estimator=RandomForestRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=20,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    max_iter=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit-transform\n",
    "imputed_array = iter_imputer.fit_transform(iter_data)\n",
    "imputed_iter_df = pd.DataFrame(imputed_array, columns=impute_cols)\n",
    "\n",
    "df['HPE (%)']  = imputed_iter_df['HPE (%)']\n",
    "df['APE (%)']  = imputed_iter_df['APE (%)']\n",
    "\n",
    "# And revert the other features to their originals (in case the imputer changed them)\n",
    "for col in set(impute_cols) - set([\"HPE (%)\", \"APE (%)\"]):\n",
    "    df[col] = original_features[col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.columns[0], axis=1)\n",
    "df = df.drop(columns=['Date', 'FTHG', 'FTAG', 'HTHG', 'HTAG', 'HTR', 'HS', 'AS', 'HST', 'AST', 'HC', 'AC', 'HF', 'AF', 'HY', 'AY', 'HR', 'AR', 'Attendance'],axis=1)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded_hometeam = pd.get_dummies(df['HomeTeam'], prefix='HomeTeam')\n",
    "one_hot_encoded_awayteam = pd.get_dummies(df['AwayTeam'], prefix='AwayTeam')\n",
    "one_hot_encoded_referee = pd.get_dummies(df['Referee'], prefix='Referee')\n",
    "one_hot_encoded_ftr = pd.get_dummies(df['FTR'], prefix='FTR')\n",
    "df = pd.concat([df, one_hot_encoded_hometeam, one_hot_encoded_awayteam, one_hot_encoded_referee, one_hot_encoded_ftr], axis=1)\n",
    "df = df.drop(columns=['HomeTeam', 'AwayTeam', 'Referee', 'FTR'], axis=1)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"HTV_missing\"] = df[\"HTV($m)\"].isnull().astype(int)\n",
    "df[\"ATV_missing\"] = df[\"ATV($m)\"].isnull().astype(int)\n",
    "\n",
    "valuation_features = [\n",
    "    \"Season\", \"Round\",\n",
    "    \"Hpts\", \"Apts\",\n",
    "    \"Home_Form_Points\", \"Away_Form_Points\",\n",
    "    \"Home_Win_Streak\", \"Away_Win_Streak\",\n",
    "    \"Home_H2H_Win_Rate\", \"Away_H2H_Win_Rate\"\n",
    "]\n",
    "\n",
    "def xgb_impute(df, target_col, feature_cols):\n",
    "    \"\"\"\n",
    "    Trains an XGBoost regressor to predict 'target_col' using 'feature_cols'.\n",
    "    Fills in missing values in 'target_col' in the original df.\n",
    "    \"\"\"\n",
    "    not_missing_mask = df[target_col].notnull()\n",
    "    missing_mask = df[target_col].isnull()\n",
    "\n",
    "    # If no missing values, just return\n",
    "    if df[missing_mask].empty:\n",
    "        print(f\"No missing values for {target_col}. Skipping XGBoost imputation.\")\n",
    "        return df\n",
    "    \n",
    "    # Prepare data\n",
    "    df_not_missing = df[not_missing_mask].copy()\n",
    "    df_missing = df[missing_mask].copy()\n",
    "\n",
    "    # Initialize XGBRegressor (tune parameters as needed)\n",
    "    xgb = XGBRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Fit only on known values\n",
    "    xgb.fit(\n",
    "        df_not_missing[feature_cols],\n",
    "        df_not_missing[target_col]\n",
    "    )\n",
    "\n",
    "    # Predict for missing\n",
    "    imputed_values = xgb.predict(df_missing[feature_cols])\n",
    "\n",
    "    # Fill back\n",
    "    df.loc[missing_mask, target_col] = imputed_values\n",
    "\n",
    "    return df\n",
    "\n",
    "df = xgb_impute(df, target_col=\"HTV($m)\", feature_cols=valuation_features)\n",
    "df = xgb_impute(df, target_col=\"ATV($m)\", feature_cols=valuation_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import keras_tuner\n",
    "\n",
    "X = df.drop(columns=['FTR_A', 'FTR_D', 'FTR_H'])\n",
    "y = df[['FTR_A', 'FTR_D', 'FTR_H']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.arange(y.shape[1]),\n",
    "    y=np.argmax(y.values, axis=1)\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.4),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(y_train.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_original = np.argmax(y_pred, axis=1)\n",
    "y_test_original = np.argmax(y_test.values, axis=1)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_original, y_pred_original))\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_original, y_pred_original))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
