{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/ALLDATA_v2.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN for computing Pos_avgs\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Select features for KNN\n",
    "features = ['HS', 'AS', 'HST', 'AST', 'Hpts', 'Apts', 'Home_Form_Points', 'Away_Form_Points']\n",
    "target_columns = [\"HTPos_avg\", \"ATPos_avg\"]\n",
    "\n",
    "# Create a mask to identify rows with missing values\n",
    "missing_mask = df[target_columns].isnull()\n",
    "\n",
    "# Add missingness indicators to the DataFrame\n",
    "for col in target_columns:\n",
    "    df[f\"{col}_missing\"] = missing_mask[col].astype(int)\n",
    "\n",
    "# Create a copy of the data for imputation\n",
    "imputation_data = df[features + target_columns].copy()\n",
    "\n",
    "# Apply KNN imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "imputed_data = knn_imputer.fit_transform(imputation_data)\n",
    "\n",
    "# Convert imputed data back to a DataFrame\n",
    "imputed_df = pd.DataFrame(imputed_data, columns=features + target_columns)\n",
    "\n",
    "# Replace only the missing values in the original DataFrame\n",
    "for col in target_columns:\n",
    "    df.loc[missing_mask[col], col] = imputed_df.loc[missing_mask[col], col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Visualize imputed vs. non-imputed values\n",
    "# plt.hist(df['HTPos_avg'], bins=30, alpha=0.5, label='Home Pos Avg')\n",
    "# plt.hist(df.loc[missing_mask['HTPos_avg'], 'HTPos_avg'], bins=30, alpha=0.5, label='Imputed Home Pos Avg')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.columns[0], axis=1)\n",
    "df = df.drop(columns=['Date', 'FTHG', 'FTAG', 'HTHG', 'HTAG', 'HTR', 'HS', 'AS', 'HST', 'AST', 'HC', 'AC', 'HF', 'AF', 'HY', 'AY', 'HR', 'AR', 'Attendance', \"HTV($m)\",\t\"ATV($m)\", \"HTPos_avg\", \"ATPos_avg\", \"HSPE (%)\", \"HPE (%)\", \"ASPE (%)\", \"APE (%)\"],axis=1)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded_hometeam = pd.get_dummies(df['HomeTeam'], prefix='HomeTeam')\n",
    "one_hot_encoded_awayteam = pd.get_dummies(df['AwayTeam'], prefix='AwayTeam')\n",
    "one_hot_encoded_referee = pd.get_dummies(df['Referee'], prefix='Referee')\n",
    "one_hot_encoded_ftr = pd.get_dummies(df['FTR'], prefix='FTR')\n",
    "df = pd.concat([df, one_hot_encoded_hometeam, one_hot_encoded_awayteam, one_hot_encoded_referee, one_hot_encoded_ftr], axis=1)\n",
    "df = df.drop(columns=['HomeTeam', 'AwayTeam', 'Referee', 'FTR'], axis=1)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import keras_tuner\n",
    "\n",
    "X = df.drop(columns=['FTR_A', 'FTR_D', 'FTR_H'])\n",
    "y = df[['FTR_A', 'FTR_D', 'FTR_H']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3912 - loss: 1.1347 - val_accuracy: 0.4808 - val_loss: 1.0380 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4687 - loss: 1.0509 - val_accuracy: 0.4938 - val_loss: 1.0113 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4788 - loss: 1.0316 - val_accuracy: 0.4658 - val_loss: 1.0263 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5011 - loss: 1.0226 - val_accuracy: 0.4904 - val_loss: 1.0133 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5207 - loss: 1.0040 - val_accuracy: 0.4733 - val_loss: 1.0277 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5070 - loss: 1.0004 - val_accuracy: 0.4808 - val_loss: 1.0153 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5335 - loss: 0.9687 - val_accuracy: 0.4664 - val_loss: 1.0251 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5314 - loss: 0.9724 - val_accuracy: 0.4548 - val_loss: 1.0605 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5607 - loss: 0.9424 - val_accuracy: 0.4493 - val_loss: 1.0349 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5876 - loss: 0.9058 - val_accuracy: 0.4514 - val_loss: 1.0287 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5779 - loss: 0.9068 - val_accuracy: 0.4438 - val_loss: 1.0444 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m167/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5997 - loss: 0.8717\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5980 - loss: 0.8737 - val_accuracy: 0.4479 - val_loss: 1.0673 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6176 - loss: 0.8383 - val_accuracy: 0.4568 - val_loss: 1.0609 - learning_rate: 5.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6336 - loss: 0.8025 - val_accuracy: 0.4582 - val_loss: 1.0758 - learning_rate: 5.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6626 - loss: 0.7611 - val_accuracy: 0.4836 - val_loss: 1.0851 - learning_rate: 5.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6647 - loss: 0.7504 - val_accuracy: 0.4418 - val_loss: 1.1172 - learning_rate: 5.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6740 - loss: 0.7264 - val_accuracy: 0.4514 - val_loss: 1.1284 - learning_rate: 5.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6808 - loss: 0.7059 - val_accuracy: 0.4500 - val_loss: 1.1628 - learning_rate: 5.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6923 - loss: 0.6881 - val_accuracy: 0.4219 - val_loss: 1.1944 - learning_rate: 5.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6951 - loss: 0.6622 - val_accuracy: 0.4527 - val_loss: 1.1679 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7195 - loss: 0.6496 - val_accuracy: 0.4521 - val_loss: 1.1996 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m173/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7298 - loss: 0.6293\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7296 - loss: 0.6296 - val_accuracy: 0.4466 - val_loss: 1.2279 - learning_rate: 5.0000e-04\n",
      "Epoch 22: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step - accuracy: 0.4819 - loss: 1.0306\n",
      "Test Accuracy: 0.49\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step\n",
      "Confusion Matrix:\n",
      "[[276  92 178]\n",
      " [131  92 217]\n",
      " [169 141 528]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.51      0.49       546\n",
      "           1       0.28      0.21      0.24       440\n",
      "           2       0.57      0.63      0.60       838\n",
      "\n",
      "    accuracy                           0.49      1824\n",
      "   macro avg       0.44      0.45      0.44      1824\n",
      "weighted avg       0.47      0.49      0.48      1824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.arange(y.shape[1]),\n",
    "    y=np.argmax(y.values, axis=1)\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.4),\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.4),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(y_train.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_original = np.argmax(y_pred, axis=1)\n",
    "y_test_original = np.argmax(y_test.values, axis=1)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_original, y_pred_original))\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_original, y_pred_original))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
