{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/ALLDATA_v2.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_head_to_head(row, matches):\n",
    "    relevant_matches = matches[(\n",
    "        ((matches['HomeTeam'] == row['HomeTeam']) & (matches['AwayTeam'] == row['AwayTeam'])) |\n",
    "        ((matches['HomeTeam'] == row['AwayTeam']) & (matches['AwayTeam'] == row['HomeTeam']))\n",
    "    )]\n",
    "    \n",
    "    home_wins = (\n",
    "        ((relevant_matches['HomeTeam'] == row['HomeTeam']) & (relevant_matches['FTR'] == 'H')) |\n",
    "        ((relevant_matches['AwayTeam'] == row['HomeTeam']) & (relevant_matches['FTR'] == 'A'))\n",
    "    ).sum()\n",
    "    \n",
    "    away_wins = (\n",
    "        ((relevant_matches['HomeTeam'] == row['AwayTeam']) & (relevant_matches['FTR'] == 'H')) |\n",
    "        ((relevant_matches['AwayTeam'] == row['AwayTeam']) & (relevant_matches['FTR'] == 'A'))\n",
    "    ).sum()\n",
    "    \n",
    "    return (home_wins - away_wins) / max(len(relevant_matches), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_directional_head_to_head(row, matches):\n",
    "    relevant_matches = matches[(matches['HomeTeam'] == row['HomeTeam']) & (matches['AwayTeam'] == row['AwayTeam'])]\n",
    "    home_wins = (relevant_matches['FTR'] == 'H').sum()\n",
    "    away_wins = (relevant_matches['FTR'] == 'A').sum()\n",
    "    return (home_wins - away_wins) / max(len(relevant_matches), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_second_h(stat):\n",
    "    if stat.count('H') > 1:\n",
    "        h_index = stat.find('H', stat.find('H') + 1)\n",
    "        return stat[:h_index] + 'A' + stat[h_index + 1:]\n",
    "    return stat.replace('H', 'A')\n",
    "\n",
    "def compute_recent_stats(team, matches, stat, num_games=5):\n",
    "    team_matches = matches[(matches['HomeTeam'] == team) | (matches['AwayTeam'] == team)].tail(num_games)\n",
    "    home_stats = team_matches[team_matches['HomeTeam'] == team][stat].sum()\n",
    "    away_stats = team_matches[team_matches['AwayTeam'] == team][replace_second_h(stat)].sum()\n",
    "    total_stats = home_stats + away_stats\n",
    "    return total_stats / num_games if len(team_matches) > 0 else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_points_per_game(team, matches, num_games=5):\n",
    "    team_matches = matches[(matches['HomeTeam'] == team) | (matches['AwayTeam'] == team)].tail(num_games)\n",
    "    points = 0\n",
    "    for _, match in team_matches.iterrows():\n",
    "        if match['HomeTeam'] == team:\n",
    "            if match['FTR'] == 'H':\n",
    "                points += 3\n",
    "            elif match['FTR'] == 'D':\n",
    "                points += 1\n",
    "        elif match['AwayTeam'] == team:\n",
    "            if match['FTR'] == 'A':\n",
    "                points += 3\n",
    "            elif match['FTR'] == 'D':\n",
    "                points += 1\n",
    "    return points / num_games if len(team_matches) > 0 else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_head_to_head_and_ppg_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Head-to-Head Features\n",
    "    df['GeneralHeadToHead'] = df.apply(lambda row: compute_head_to_head(row, df[:row.name]), axis=1)\n",
    "    df['DirectionalHeadToHead'] = df.apply(lambda row: compute_directional_head_to_head(row, df[:row.name]), axis=1)\n",
    "\n",
    "    # Recent Performance Metrics\n",
    "    stats_to_average = ['HS', 'HST', 'HC', 'FTHG', 'HTHG', 'HF', 'HY', 'HR']\n",
    "    for stat in stats_to_average:\n",
    "        df[f'Home_{stat}_Avg'] = df.apply(lambda row: compute_recent_stats(row['HomeTeam'], df[:row.name], stat), axis=1)\n",
    "        df[f'Away_{replace_second_h(stat)}_Avg'] = df.apply(lambda row: compute_recent_stats(row['AwayTeam'], df[:row.name], stat), axis=1)\n",
    "\n",
    "    # Points Per Game Feature\n",
    "    df['Home_PPG'] = df.apply(lambda row: compute_points_per_game(row['HomeTeam'], df[:row.name]), axis=1)\n",
    "    df['Away_PPG'] = df.apply(lambda row: compute_points_per_game(row['AwayTeam'], df[:row.name]), axis=1)\n",
    "\n",
    "    # Fill Default Values\n",
    "    df.fillna({\n",
    "        'GeneralHeadToHead': 0,\n",
    "        'DirectionalHeadToHead': 0,\n",
    "        'Home_HS_Avg': 14,\n",
    "        'Away_AS_Avg': 11,\n",
    "        'Home_HST_Avg': 6,\n",
    "        'Away_AST_Avg': 5,\n",
    "        'Home_HC_Avg': 6,\n",
    "        'Away_AC_Avg': 5,\n",
    "        'Home_FTHG_Avg': 2,\n",
    "        'Away_FTAG_Avg': 1,\n",
    "        'Home_HTHG_Avg': 1,\n",
    "        'Away_HTAG_Avg': 1,\n",
    "        'Home_HF_Avg': 11,\n",
    "        'Away_AF_Avg': 12,\n",
    "        'Home_HY_Avg': 1,\n",
    "        'Away_AY_Avg': 2,\n",
    "        'Home_HR_Avg': 0,\n",
    "        'Away_AR_Avg': 0,\n",
    "        'Home_PPG': 1.5,\n",
    "        'Away_PPG': 1.2\n",
    "    }, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = compute_head_to_head_and_ppg_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN for imputing pos_avg data\n",
    "\n",
    "def impute_pos_avg():\n",
    "\n",
    "    # Select features for KNN\n",
    "    features = ['HS', 'AS', 'HST', 'AST', 'Hpts', 'Apts', 'Home_Form_Points', 'Away_Form_Points']\n",
    "    target_columns = [\"HTPos_avg\", \"ATPos_avg\"]\n",
    "\n",
    "    missing_mask = df[target_columns].isnull()\n",
    "\n",
    "    for col in target_columns:\n",
    "        df[f\"{col}_missing\"] = missing_mask[col].astype(int)\n",
    "\n",
    "    imputation_data = df[features + target_columns].copy()\n",
    "\n",
    "    knn_imputer = KNNImputer(n_neighbors=5)\n",
    "    imputed_data = knn_imputer.fit_transform(imputation_data)\n",
    "\n",
    "    imputed_df = pd.DataFrame(imputed_data, columns=features + target_columns)\n",
    "\n",
    "    for col in target_columns:\n",
    "        df.loc[missing_mask[col], col] = imputed_df.loc[missing_mask[col], col]\n",
    "\n",
    "impute_pos_avg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK IF THIS IS ACTUALLY USEFUL TO INCLUDE, FEELS LIKE ACCURACY DIPS SOMETIMES BECAUSE OF THIS AS THERE'S AN OBSERVED FLUCTUATION IN ACCURACY BETWEEN 0.46 AND 0.53 \n",
    "# Use a combination of random forest regressor and an iterative imputer to get missing values for HSPE and ASPE\n",
    "\n",
    "# Random Forest Regression\n",
    "def random_forest_impute(df, target_col, feature_cols):\n",
    "    \"\"\"\n",
    "    Trains a RandomForestRegressor to predict 'target_col' using 'feature_cols'.\n",
    "    Fills in missing values in 'target_col' in the original df.\n",
    "    \"\"\"\n",
    "    not_missing_mask = df[target_col].notnull()\n",
    "    missing_mask = df[target_col].isnull()\n",
    "\n",
    "    df_not_missing = df[not_missing_mask]\n",
    "    df_missing = df[missing_mask]\n",
    "\n",
    "    if df_missing.empty:\n",
    "        print(f\"No missing values for {target_col}. Skipping RF imputation.\")\n",
    "        return df\n",
    "\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=20,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    rf.fit(df_not_missing[feature_cols], df_not_missing[target_col])\n",
    "    imputed_values = rf.predict(df_missing[feature_cols])\n",
    "    df.loc[missing_mask, target_col] = imputed_values\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example feature set for random forest:\n",
    "rf_features = [\n",
    "    'Hpts', 'Apts', \n",
    "    'Home_Form_Points', 'Away_Form_Points',\n",
    "    'Home_H2H_Win_Rate', 'Away_H2H_Win_Rate',\n",
    "    'HTS', 'ATS'\n",
    "]\n",
    "\n",
    "df[\"HSPE_missing\"] = df[\"HSPE (%)\"].isnull().astype(int)\n",
    "df[\"ASPE_missing\"] = df[\"ASPE (%)\"].isnull().astype(int)\n",
    "\n",
    "df = random_forest_impute(\n",
    "    df=df, \n",
    "    target_col='HSPE (%)', \n",
    "    feature_cols=rf_features\n",
    ")\n",
    "\n",
    "df = random_forest_impute(\n",
    "    df=df, \n",
    "    target_col='ASPE (%)', \n",
    "    feature_cols=rf_features\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest followed by iterated imputation to be able to get missing values for HPE AND APE and include them as features\n",
    "df[\"HPE_missing\"] = df[\"HPE (%)\"].isnull().astype(int)\n",
    "df[\"APE_missing\"] = df[\"APE (%)\"].isnull().astype(int)\n",
    "\n",
    "rf_features_for_hpe_ape = [\n",
    "    'Hpts', 'Apts',\n",
    "    'Home_Form_Points', 'Away_Form_Points',\n",
    "    'Home_H2H_Win_Rate', 'Away_H2H_Win_Rate',\n",
    "    'HTS', 'ATS',\n",
    "]\n",
    "\n",
    "df = random_forest_impute(\n",
    "    df=df,\n",
    "    target_col='HPE (%)',\n",
    "    feature_cols=rf_features_for_hpe_ape\n",
    ")\n",
    "\n",
    "df = random_forest_impute(\n",
    "    df=df,\n",
    "    target_col='APE (%)',\n",
    "    feature_cols=rf_features_for_hpe_ape\n",
    ")\n",
    "\n",
    "impute_cols = (\n",
    "    rf_features_for_hpe_ape + \n",
    "    [\"HPE (%)\", \"APE (%)\"]\n",
    ")\n",
    "\n",
    "impute_cols = list(dict.fromkeys(impute_cols))\n",
    "\n",
    "iter_data = df[impute_cols].copy()\n",
    "\n",
    "original_features = df[impute_cols].copy()  \n",
    "\n",
    "# Initialize IterativeImputer\n",
    "iter_imputer = IterativeImputer(\n",
    "    estimator=RandomForestRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=20,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    max_iter=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit-transform\n",
    "imputed_array = iter_imputer.fit_transform(iter_data)\n",
    "imputed_iter_df = pd.DataFrame(imputed_array, columns=impute_cols)\n",
    "\n",
    "df['HPE (%)']  = imputed_iter_df['HPE (%)']\n",
    "df['APE (%)']  = imputed_iter_df['APE (%)']\n",
    "\n",
    "# And revert the other features to their originals (in case the imputer changed them)\n",
    "for col in set(impute_cols) - set([\"HPE (%)\", \"APE (%)\"]):\n",
    "    df[col] = original_features[col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.columns[0], axis=1)\n",
    "df = df.drop(columns=['Date', 'FTHG', 'FTAG', 'HTHG', 'HTAG', 'HTR', 'HS', 'AS', 'HST', 'AST', 'HC', 'AC', 'HF', 'AF', 'HY', 'AY', 'HR', 'AR', 'Attendance'],axis=1)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded_hometeam = pd.get_dummies(df['HomeTeam'], prefix='HomeTeam')\n",
    "one_hot_encoded_awayteam = pd.get_dummies(df['AwayTeam'], prefix='AwayTeam')\n",
    "one_hot_encoded_referee = pd.get_dummies(df['Referee'], prefix='Referee')\n",
    "one_hot_encoded_ftr = pd.get_dummies(df['FTR'], prefix='FTR')\n",
    "df = pd.concat([df, one_hot_encoded_hometeam, one_hot_encoded_awayteam, one_hot_encoded_referee, one_hot_encoded_ftr], axis=1)\n",
    "df = df.drop(columns=['HomeTeam', 'AwayTeam', 'Referee', 'FTR'], axis=1)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"HTV_missing\"] = df[\"HTV($m)\"].isnull().astype(int)\n",
    "df[\"ATV_missing\"] = df[\"ATV($m)\"].isnull().astype(int)\n",
    "\n",
    "valuation_features = [\n",
    "    \"Season\", \"Round\",\n",
    "    \"Hpts\", \"Apts\",\n",
    "    \"Home_Form_Points\", \"Away_Form_Points\",\n",
    "    \"Home_Win_Streak\", \"Away_Win_Streak\",\n",
    "    \"Home_H2H_Win_Rate\", \"Away_H2H_Win_Rate\"\n",
    "]\n",
    "\n",
    "def xgb_impute(df, target_col, feature_cols):\n",
    "    not_missing_mask = df[target_col].notnull()\n",
    "    missing_mask = df[target_col].isnull()\n",
    "\n",
    "    if df[missing_mask].empty:\n",
    "        return df\n",
    "    \n",
    "    df_not_missing = df[not_missing_mask].copy()\n",
    "    df_missing = df[missing_mask].copy()\n",
    "\n",
    "    xgb = XGBRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    xgb.fit(\n",
    "        df_not_missing[feature_cols],\n",
    "        df_not_missing[target_col]\n",
    "    )\n",
    "\n",
    "    imputed_values = xgb.predict(df_missing[feature_cols])\n",
    "\n",
    "    df.loc[missing_mask, target_col] = imputed_values\n",
    "\n",
    "    return df\n",
    "\n",
    "df = xgb_impute(df, target_col=\"HTV($m)\", feature_cols=valuation_features)\n",
    "df = xgb_impute(df, target_col=\"ATV($m)\", feature_cols=valuation_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['FTR_A', 'FTR_D', 'FTR_H'])\n",
    "y = df[['FTR_A', 'FTR_D', 'FTR_H']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.arange(y.shape[1]),\n",
    "    y=np.argmax(y.values, axis=1)\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(y_train.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_original = np.argmax(y_pred, axis=1)\n",
    "y_test_original = np.argmax(y_test.values, axis=1)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_original, y_pred_original))\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_original, y_pred_original))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
